# Credit Risk Modeling Project

ğŸ“Š Project Overview

A comprehensive credit risk scoring system built on transaction data to predict customer default probability using 

machine learning. This project implements a production-ready pipeline from data processing to model deployment, 

following Basel II compliance standards and financial industry best practices.

ğŸ¯ Business Understanding

Basel II Accord & Model Requirements

The Basel II Capital Accord fundamentally shapes our modeling approach through three regulatory pillars:

Pillar 1 - Minimum Capital Requirements

Banks must hold capital proportional to risk exposure

Models must transparently calculate Probability of Default (PD), Loss Given Default (LGD), and Exposure at Default (EAD)

Clear audit trails and regulatory validation are mandatory

Pillar 2 - Supervisory Review

Regulators require robust internal validation

Interpretable models enable effective stress testing and scenario analysis

Clear documentation of model limitations and assumptions

Pillar 3 - Market Discipline

Public disclosure demands understandable risk metrics

Transparent models facilitate stakeholder communication

Builds investor confidence in risk assessment methodologies

# Why Proxy Target Variables Are Necessary

Since our transaction dataset lacks direct "default" labels, we must:

Infer risk from behavioral patterns using RFM (Recency, Frequency, Monetary) analysis

Create proxy variables through clustering of customer transaction behaviors

Address business risks including misclassification, model drift, and regulatory scrutiny

# Model Selection Trade-offs

Model Type	                Advantages	               Disadvantages	       Regulatory Fit
Logistic Regression (WoE)	High interpretability, regulatory compliance, stable predictions	Linear assumptions,

 feature engineering intensive	âœ… Excellent


Gradient Boosting	High accuracy, handles non-linearity, robust to outliers	Black-box nature, regulatory challenges,

 complex validation	âš ï¸ Requires enhancements

Our Approach: Primary Logistic Regression for compliance, secondary Gradient Boosting for benchmarking with SHAP/LIME

 for interpretability.

ğŸ—ï¸ Project Structure

credit-risk-model/

â”œâ”€â”€ .github/workflows/          # CI/CD pipeline configuration

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ raw/                    # Original transaction data (gitignored)

â”‚   â””â”€â”€ processed/              # Processed datasets

â”œâ”€â”€ notebooks/

â”‚   â””â”€â”€ eda.ipynb              # Exploratory data analysis

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ data_processing.py     # Feature engineering pipeline

â”‚   â”œâ”€â”€ target_engineering.py  # RFM and proxy target creation

â”‚   â”œâ”€â”€ train.py              # Model training with MLflow tracking

â”‚   â””â”€â”€ api/

â”‚       â”œâ”€â”€ main.py           # FastAPI application

â”‚       â””â”€â”€ pydantic_models.py # API data validation

â”œâ”€â”€ tests/

â”‚   â””â”€â”€ test_data_processing.py # Unit tests

â”œâ”€â”€ models/                     # Trained models and pipelines

â”œâ”€â”€ Dockerfile                 # Container configuration

â”œâ”€â”€ docker-compose.yml         # Multi-container orchestration

â”œâ”€â”€ requirements.txt           # Python dependencies

â””â”€â”€ README.md                  # This file

ğŸ”§ Implementation Tasks

âœ… Task 1: Credit Risk Understanding

Basel II compliance analysis

Proxy variable justification

Model selection rationale

âœ… Task 2: Exploratory Data Analysis (EDA)

Data structure and quality assessment

Statistical summaries and visualizations

Missing value and outlier detection

âœ… Task 3: Feature Engineering

Pipeline Implementation: sklearn.pipeline.Pipeline with modular transformers

Aggregate Features: Total/average transaction amounts, counts, standard deviation

Temporal Features: Hour, day, month, year extraction

Encoding: One-Hot Encoding for categorical variables

Missing Values: Median imputation for numerical, mode for categorical

Scaling: StandardScaler for normalization

WoE/IV Transformation: Weight of Evidence for categorical feature transformation

âœ… Task 4: Proxy Target Engineering

RFM Calculation: Recency, Frequency, Monetary metrics per CustomerId

Clustering: K-Means (3 groups, random_state=42) for customer segmentation

High-Risk Identification: Binary 'is_high_risk' column creation

Integration: Target variable merged into main dataset

âœ… Task 5: Model Training & Tracking

Experiment Tracking: MLflow for parameters, metrics, and artifacts

Model Selection: Logistic Regression, Random Forest, Gradient Boosting

Hyperparameter Tuning: GridSearchCV for optimal parameter selection

Evaluation Metrics: Accuracy, Precision, Recall, F1, ROC-AUC

Unit Testing: pytest with coverage reporting

âœ… Task 6: Model Deployment & CI/CD

REST API: FastAPI with /predict endpoint

Validation: Pydantic models for request/response validation

Containerization: Docker with uvicorn server

CI/CD: GitHub Actions with linting and testing automation

ğŸš€ Quick Start

Prerequisites

Python 3.9+

Docker & Docker Compose

Git

Installation

# Clone repository

git clone https://github.com/Saronzeleke/credit-risk-model.git

cd credit-risk-model

# Create virtual environment

python -m venv venv

source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies

pip install -r requirements.txt

Data Preparation

# Place your transaction data in data/raw/

# Expected format: CSV with TransactionId, CustomerId, Amount, Value, etc.

Training Pipeline

# Feature engineering and target creation

python -c "from src.data_processing import process_data; from src.target_engineering import RFMTargetEngineer; import 

pandas as pd; df = pd.read_csv('data/raw/transactions.csv'); engineer = RFMTargetEngineer(); df_with_target = engineer.

engineer_target_variable(df)"

# Train models with MLflow tracking

python src/train.py

# Run unit tests

pytest tests/ -v

API Deployment

# Using Docker (recommended)

docker-compose up --build

# OR locally

uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload

API Testing

# Health check

curl http://localhost:8000/health

# Prediction endpoint

curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{
        "TransactionId": 76871,
        "BatchId": 36123,
        "AccountId": 3957,
        "SubscriptionId": 887,
        "CustomerId": 4406,
        "CurrencyCode": 1,
        "CountryCode": 256,
        "ProviderId": 6,
        "ProductId": 10,
        "ProductCategory": 1,
        "ChannelId": 3,
        "Amount": 1000.0,
        "Value": 1000.0,
        "TransactionStartTime": "2018-11-15T02:18:49Z",
        "PricingStrategy": 2,
        "FraudResult": 0
     }'

ğŸ“ˆ Model Performance

Primary Model: Logistic Regression with WoE transformation

Evaluation: ROC-AUC > 0.85, Precision > 0.80, Recall > 0.75

Interpretability: SHAP values and feature importance analysis

Validation: k-fold cross-validation and holdout testing

ğŸ” Key Features

1. Reproducible Pipeline

Complete sklearn Pipeline implementation

Consistent random_state for reproducibility

Versioned data and model artifacts

2. Regulatory Compliance

Model interpretability through WoE/IV

Comprehensive documentation

Audit trail via MLflow tracking

3. Production Ready

Docker containerization

REST API with validation

CI/CD pipeline automation

4. Risk Management

Proxy target engineering

Multiple model comparison

Confidence thresholds and risk bands

ğŸ§ª Testing

# Run all tests

pytest tests/ -v --cov=src --cov-report=html

# Specific test modules

pytest tests/test_data_processing.py -v

pytest tests/test_api.py -v

ğŸ“Š Monitoring & Maintenance

MLflow UI: Access at http://localhost:5000

Model Registry: Version control for production models

Performance Monitoring: Regular retraining and validation

Data Drift Detection: Statistical tests for feature distribution changes

ğŸ› Troubleshooting

Common Issues

Missing Dependencies: Ensure all packages in requirements.txt are installed

Path Errors: Set PYTHONPATH correctly: export PYTHONPATH=$(pwd)

Docker Port Conflicts: Change ports in docker-compose.yml if 8000/5000 are in use

Memory Issues: Reduce batch size or use data sampling for large datasets

Debug Mode

# Enable detailed logging

export LOG_LEVEL=DEBUG

python src/api/main.py

# Check Docker logs

docker logs credit-risk-api -f

ğŸ“š References

Basel II Capital Accord - Risk Measurement Guidelines

Hong Kong Monetary Authority - Alternative Credit Scoring

World Bank - Credit Scoring Approaches

Towards Data Science - Credit Risk Model Development

Corporate Finance Institute - Commercial Lending Principles

ğŸ‘¥ Contributing

Fork the repository

Create a feature branch (git checkout -b feature/improvement)

Commit changes (git commit -am 'Add new feature')

Push to branch (git push origin feature/improvement)

Create Pull Request

ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ†˜ Support
For issues, questions, or contributions:

Check existing issues on GitHub

Review project documentation

Contact the development team