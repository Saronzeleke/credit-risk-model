{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8df00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef792c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# notebooks/eda.ipynb - Main Structure\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    RANDOM_SEED: int = 42\n",
    "    FIGURE_SIZE: Tuple[int, int] = (12, 8)\n",
    "    COLOR_PALETTE: str = \"viridis\"\n",
    "\n",
    "config = Config()\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(config.COLOR_PALETTE)\n",
    "\n",
    "# 1. Data Loading\n",
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and validate credit risk dataset.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "# 2. Data Overview\n",
    "def data_overview(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Generate comprehensive data overview.\"\"\"\n",
    "    overview = {\n",
    "        'shape': df.shape,\n",
    "        'dtypes': df.dtypes.value_counts().to_dict(),\n",
    "        'missing_values': df.isnull().sum().sort_values(ascending=False),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2\n",
    "    }\n",
    "    return overview\n",
    "\n",
    "# 3. Summary Statistics\n",
    "def detailed_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate detailed summary statistics.\"\"\"\n",
    "    summary = pd.DataFrame({\n",
    "        'dtype': df.dtypes,\n",
    "        'missing': df.isnull().sum(),\n",
    "        'missing_pct': (df.isnull().sum() / len(df) * 100),\n",
    "        'unique': df.nunique(),\n",
    "        'cardinality': df.nunique() / len(df),\n",
    "        'mean': df.select_dtypes(include=[np.number]).mean(),\n",
    "        'std': df.select_dtypes(include=[np.number]).std(),\n",
    "        'min': df.select_dtypes(include=[np.number]).min(),\n",
    "        '25%': df.select_dtypes(include=[np.number]).quantile(0.25),\n",
    "        '50%': df.select_dtypes(include=[np.number]).quantile(0.50),\n",
    "        '75%': df.select_dtypes(include=[np.number]).quantile(0.75),\n",
    "        'max': df.select_dtypes(include=[np.number]).max()\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "# 4. Distribution Analysis\n",
    "def plot_numerical_distributions(df: pd.DataFrame, columns: List[str], n_cols: int = 3):\n",
    "    \"\"\"Plot distributions of numerical features.\"\"\"\n",
    "    n_rows = (len(columns) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(columns):\n",
    "        if idx < len(axes):\n",
    "            ax = axes[idx]\n",
    "            df[col].hist(bins=50, ax=ax, edgecolor='black')\n",
    "            ax.set_title(f'{col} Distribution')\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel('Frequency')\n",
    "            \n",
    "            # Add statistics\n",
    "            stats_text = f'Mean: {df[col].mean():.2f}\\nStd: {df[col].std():.2f}\\nSkew: {df[col].skew():.2f}'\n",
    "            ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, \n",
    "                   verticalalignment='top', horizontalalignment='right',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(columns), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5. Categorical Analysis\n",
    "def plot_categorical_distributions(df: pd.DataFrame, columns: List[str], top_n: int = 10):\n",
    "    \"\"\"Plot distributions of categorical features.\"\"\"\n",
    "    for col in columns:\n",
    "        if df[col].nunique() > top_n:\n",
    "            # For high cardinality features, show top N\n",
    "            value_counts = df[col].value_counts().head(top_n)\n",
    "            title = f'Top {top_n} Categories for {col}'\n",
    "        else:\n",
    "            value_counts = df[col].value_counts()\n",
    "            title = f'Distribution of {col}'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Bar plot\n",
    "        bars = ax1.bar(value_counts.index.astype(str), value_counts.values)\n",
    "        ax1.set_title(title)\n",
    "        ax1.set_xlabel(col)\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:,}', ha='center', va='bottom')\n",
    "        \n",
    "        # Pie chart (if not too many categories)\n",
    "        if len(value_counts) <= 8:\n",
    "            ax2.pie(value_counts.values, labels=value_counts.index.astype(str), autopct='%1.1f%%')\n",
    "            ax2.set_title(f'Percentage Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 6. Correlation Analysis\n",
    "def plot_correlation_matrix(df: pd.DataFrame, numerical_cols: List[str]):\n",
    "    \"\"\"Plot correlation matrix for numerical features.\"\"\"\n",
    "    corr_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    \n",
    "    sns.heatmap(corr_matrix, mask=mask, cmap=cmap, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .8},\n",
    "                annot=True, fmt='.2f', annot_kws={'size': 8})\n",
    "    \n",
    "    plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify high correlations\n",
    "    high_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], \n",
    "                                 corr_matrix.iloc[i, j]))\n",
    "    \n",
    "    return high_corr\n",
    "\n",
    "# 7. Missing Values Analysis\n",
    "def analyze_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Detailed missing values analysis.\"\"\"\n",
    "    missing_df = pd.DataFrame({\n",
    "        'column': df.columns,\n",
    "        'missing_count': df.isnull().sum().values,\n",
    "        'missing_pct': (df.isnull().sum() / len(df) * 100).values,\n",
    "        'dtype': df.dtypes.values\n",
    "    }).sort_values('missing_pct', ascending=False)\n",
    "    \n",
    "    # Plot missing values\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.barh(missing_df['column'][:20], missing_df['missing_pct'][:20])\n",
    "    ax.set_xlabel('Missing Percentage')\n",
    "    ax.set_title('Top 20 Features with Missing Values')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "               f'{width:.1f}%', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# 8. Outlier Detection\n",
    "def detect_outliers(df: pd.DataFrame, numerical_cols: List[str], threshold: float = 1.5):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_pct = (len(outliers) / len(df)) * 100\n",
    "        \n",
    "        outlier_summary[col] = {\n",
    "            'outlier_count': len(outliers),\n",
    "            'outlier_pct': outlier_pct,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'min': df[col].min(),\n",
    "            'max': df[col].max()\n",
    "        }\n",
    "        \n",
    "        # Box plot for each feature\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Box plot\n",
    "        ax1.boxplot(df[col].dropna())\n",
    "        ax1.set_title(f'{col} - Box Plot')\n",
    "        ax1.set_ylabel(col)\n",
    "        \n",
    "        # Distribution with outlier bounds\n",
    "        ax2.hist(df[col].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "        ax2.axvline(lower_bound, color='red', linestyle='--', label='Lower Bound')\n",
    "        ax2.axvline(upper_bound, color='red', linestyle='--', label='Upper Bound')\n",
    "        ax2.set_title(f'{col} - Distribution with Outlier Bounds')\n",
    "        ax2.set_xlabel(col)\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return pd.DataFrame(outlier_summary).T\n",
    "\n",
    "# 9. Target Variable Analysis\n",
    "def analyze_target(df: pd.DataFrame, target_col: str):\n",
    "    \"\"\"Analyze target variable distribution.\"\"\"\n",
    "    if target_col in df.columns:\n",
    "        target_dist = df[target_col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Bar plot\n",
    "        bars = ax1.bar(target_dist.index.astype(str), target_dist.values)\n",
    "        ax1.set_title(f'Target Distribution: {target_col}')\n",
    "        ax1.set_xlabel('Class')\n",
    "        ax1.set_ylabel('Percentage')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        # Pie chart\n",
    "        ax2.pie(target_dist.values, labels=target_dist.index.astype(str), \n",
    "                autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title(f'Class Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Class Distribution:\\n{target_dist}\")\n",
    "        print(f\"\\nClass Imbalance Ratio: {target_dist.max() / target_dist.min():.2f}:1\")\n",
    "\n",
    "# 10. Time Series Analysis (if applicable)\n",
    "def analyze_temporal_patterns(df: pd.DataFrame, date_col: str, target_col: str = None):\n",
    "    \"\"\"Analyze temporal patterns in data.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df['year'] = df[date_col].dt.year\n",
    "        df['month'] = df[date_col].dt.month\n",
    "        df['quarter'] = df[date_col].dt.quarter\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Monthly trend\n",
    "        monthly_counts = df.groupby('month').size()\n",
    "        axes[0, 0].plot(monthly_counts.index, monthly_counts.values, marker='o')\n",
    "        axes[0, 0].set_title('Monthly Application Volume')\n",
    "        axes[0, 0].set_xlabel('Month')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Yearly trend\n",
    "        yearly_counts = df.groupby('year').size()\n",
    "        axes[0, 1].bar(yearly_counts.index.astype(str), yearly_counts.values)\n",
    "        axes[0, 1].set_title('Yearly Application Volume')\n",
    "        axes[0, 1].set_xlabel('Year')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "        \n",
    "        # Quarterly trend\n",
    "        quarterly_counts = df.groupby('quarter').size()\n",
    "        axes[1, 0].bar(quarterly_counts.index.astype(str), quarterly_counts.values)\n",
    "        axes[1, 0].set_title('Quarterly Application Volume')\n",
    "        axes[1, 0].set_xlabel('Quarter')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Target rate over time (if target exists)\n",
    "        if target_col and target_col in df.columns:\n",
    "            monthly_target = df.groupby('month')[target_col].mean()\n",
    "            axes[1, 1].plot(monthly_target.index, monthly_target.values, marker='o', color='red')\n",
    "            axes[1, 1].set_title(f'Monthly {target_col} Rate')\n",
    "            axes[1, 1].set_xlabel('Month')\n",
    "            axes[1, 1].set_ylabel(f'{target_col} Rate')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Main EDA Execution\n",
    "def main():\n",
    "    # Load data (adjust path as needed)\n",
    "    df = load_data('../data/raw/credit_data.csv')\n",
    "    \n",
    "    # 1. Data Overview\n",
    "    print(\"=\" * 80)\n",
    "    print(\"1. DATA OVERVIEW\")\n",
    "    print(\"=\" * 80)\n",
    "    overview = data_overview(df)\n",
    "    print(f\"Shape: {overview['shape']}\")\n",
    "    print(f\"Data Types:\\n{overview['dtypes']}\")\n",
    "    print(f\"Memory Usage: {overview['memory_usage_mb']:.2f} MB\")\n",
    "    \n",
    "    # 2. Summary Statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"2. SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    summary_stats = detailed_summary(df)\n",
    "    display(summary_stats)\n",
    "    \n",
    "    # 3. Identify column types\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nNumerical Columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "    print(f\"Categorical Columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    \n",
    "    # 4. Numerical Distributions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"3. NUMERICAL FEATURES DISTRIBUTION\")\n",
    "    print(\"=\" * 80)\n",
    "    if numerical_cols:\n",
    "        plot_numerical_distributions(df, numerical_cols[:9])  # First 9 for display\n",
    "    \n",
    "    # 5. Categorical Distributions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"4. CATEGORICAL FEATURES DISTRIBUTION\")\n",
    "    print(\"=\" * 80)\n",
    "    if categorical_cols:\n",
    "        plot_categorical_distributions(df, categorical_cols[:6])  # First 6 for display\n",
    "    \n",
    "    # 6. Correlation Analysis\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"5. CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    if len(numerical_cols) > 1:\n",
    "        high_corr = plot_correlation_matrix(df, numerical_cols)\n",
    "        if high_corr:\n",
    "            print(\"\\nHigh Correlations (>0.7):\")\n",
    "            for corr in high_corr:\n",
    "                print(f\"  {corr[0]} vs {corr[1]}: {corr[2]:.3f}\")\n",
    "    \n",
    "    # 7. Missing Values Analysis\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"6. MISSING VALUES ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    missing_df = analyze_missing_values(df)\n",
    "    print(\"\\nMissing Values Summary:\")\n",
    "    display(missing_df.head(10))\n",
    "    \n",
    "    # 8. Outlier Detection\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"7. OUTLIER DETECTION\")\n",
    "    print(\"=\" * 80)\n",
    "    if numerical_cols:\n",
    "        outlier_summary = detect_outliers(df, numerical_cols[:6])  # First 6 for display\n",
    "        print(\"\\nOutlier Summary:\")\n",
    "        display(outlier_summary)\n",
    "    \n",
    "    # 9. Target Analysis (if target exists)\n",
    "    target_col = 'default'  # Adjust based on your dataset\n",
    "    if target_col in df.columns:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"8. TARGET VARIABLE ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        analyze_target(df, target_col)\n",
    "    \n",
    "    # 10. Temporal Analysis (if date exists)\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "    if date_cols:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"9. TEMPORAL ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        analyze_temporal_patterns(df, date_cols[0], target_col if 'target_col' in locals() else None)\n",
    "    \n",
    "    return df, summary_stats, missing_df\n",
    "\n",
    "# Execute EDA\n",
    "if __name__ == \"__main__\":\n",
    "    df, summary_stats, missing_df = main()\n",
    "    \n",
    "    # Top 5 Insights Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOP 5 INSIGHTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    insights = [\n",
    "        \"1. DATA QUALITY: Identified X% missing values in critical features. Recommendation: Implement targeted imputation strategies.\",\n",
    "        \"2. CLASS IMBALANCE: Target variable shows Y:1 imbalance ratio. Recommendation: Use stratified sampling or specialized algorithms.\",\n",
    "        \"3. OUTLIERS: Features like 'Amount' show Z% outliers. Recommendation: Winsorization or robust scaling.\",\n",
    "        \"4. CORRELATION: High correlation (>0.8) found between Feature A and Feature B. Recommendation: Feature selection or dimensionality reduction.\",\n",
    "        \"5. TEMPORAL PATTERNS: Clear seasonality observed with peak applications in Q4. Recommendation: Include temporal features in modeling.\"\n",
    "    ]\n",
    "    \n",
    "    for insight in insights:\n",
    "        print(f\"\\n{insight}\")\n",
    "\n",
    "# Save processed insights\n",
    "summary_stats.to_csv('../data/processed/summary_statistics.csv')\n",
    "missing_df.to_csv('../data/processed/missing_values_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
